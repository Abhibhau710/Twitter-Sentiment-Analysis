# -*- coding: utf-8 -*-
"""sentiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eRsiEn-8ylYz_-BJ1H-1XVKUWByrPpPI
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow import keras

data = pd.read_csv("/content/Twitter_Data.csv")
data.head(5)

data = data.sample(frac = 1).reset_index(drop = True)
data.head(5)

labels = pd.get_dummies(data.category)
labels.columns = ["negative", "neutral", "positive"]
labels.head(5)

data = data.drop(columns = "category")
data.head(3)

from keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
tokenizer = Tokenizer(num_words = 8150, lower = True, split = " ", oov_token = "~")
# Check for missing values in the "clean_text" column
missing_values = data["clean_text"].isnull().sum()

if missing_values > 0:
    # Handle missing values (e.g., replace with an empty string)
    data["clean_text"].fillna("", inplace=True)

# Fit the tokenizer on the clean_text data
tokenizer.fit_on_texts(data["clean_text"])
tokenizer.fit_on_texts(data["clean_text"])



word_index = tokenizer.word_index
len(word_index)

print(list(word_index.keys())[:100]) #first 100 tokens in word_index

data["clean_text"] = tokenizer.texts_to_sequences(data["clean_text"])

data.head(3)

len(data.clean_text[0]), len(data.clean_text[1]), len(data.clean_text[2])
#length of the sequences are different

tweets = pad_sequences(data["clean_text"]) #padding the sequences to get same shapes

tweets[0].shape, tweets[1].shape, tweets[2].shape

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(tweets, labels, test_size = 0.15)
X_train.shape, y_train.shape, X_test.shape, y_test.shape

X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.2)

print(f"""
Training set: tweets = {X_train.shape}, labels = {y_train.shape},
Validation set: tweets = {X_valid.shape}, labels = {y_valid.shape},
Test set: tweets = {X_test.shape}, labels = {y_test.shape}
""")

model = keras.models.Sequential([
    keras.layers.Embedding(input_dim = 8150, output_dim = 32),
    keras.layers.GRU(128),
    keras.layers.Dense(128, activation = "leaky_relu", kernel_initializer = "he_normal", kernel_regularizer = "l1"),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(3, activation = "softmax", kernel_initializer = "glorot_normal")
])
model.summary()

model.compile(optimizer = 'adam',loss = 'categorical_crossentropy',metrics = ['accuracy'])
history = model.fit(
    X_train, y_train,
    epochs = 10, validation_data = (X_valid, y_valid),
    callbacks = [keras.callbacks.EarlyStopping(patience = 3, restore_best_weights = True)]
)

model.evaluate(X_test, y_test)

model.save('model.h5')

def predict_sentiment(sentence):
    # Tokenize the sentence
    sentence_seq = tokenizer.texts_to_sequences([sentence])

    # Pad the sequence
    sentence_seq = pad_sequences(sentence_seq, maxlen=tweets.shape[1])

    # Predict the sentiment
    prediction = model.predict(sentence_seq)

    # Get the predicted label
    labels = ["negative", "neutral", "positive"]
    predicted_label = labels[np.argmax(prediction)]

    return predicted_label

sentence = "It was a good day"
predicted_sentiment = predict_sentiment(sentence)
print("Predicted Sentiment:", predicted_sentiment)